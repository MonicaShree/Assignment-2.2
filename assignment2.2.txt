1. Explain HDFS 


   * HDFS is a java based file system . 

   * Hadoop distributed file system is highly fault tolerant and designed using low-cost hardware . 
 
   * Data in a hadoop cluster is broken down into smaller pieces and distributed throughout the cluster . 

   * The files are stored in redundant fashion to reduce the failure .

   * HDFS provides parallel processing and scalability . 

   * HDFS is built to support applications with large data sets with individual files that reach upto terabytes .
 
   * It uses master and slave architecture . The name node is the master and the data nodes are the slaves . 



2. Explain Hadoop Cluster 
   
  * Hadoop cluster is a special type of computational  cluster designed for storing and analyzing huge amounts of     unstructured data in a distributed computing environment .

  * It is comprised of three different node types namely master , worker and client nodes . 

  * Master node stores the data in HDFS and running parallel computations . 

  * Worker node is responsible for storing data and running computations .   

  * Client node loads data into cluster, it describes how data should be stored and processed . 

  * It speeds up the processing of data analysis . 



3. Explain HDFS Blocks 
  
 *The data in HDFS are stored in terms of blocks . 

 *The default size of HDFS block is 64MB.

 *The files are split into blocks and then stored in hadoop file system.

 *The blocks are easy to replicate between data nodes.
 
 *It is fault tolerent and is highly available. 

 *The size of the file is less than the HDFS block size.the file does not occupy the complete block storage.




